{"cells":[{"cell_type":"markdown","source":["第一步：安装和导入必要包"],"metadata":{"id":"uj7dQUHam05d"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install evaluate"],"metadata":{"collapsed":true,"id":"C2t_eoaImxz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"iw1v_mUqiyHq"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, import TrainingArguments, Trainer\n","import numpy as np\n","import evaluate"]},{"cell_type":"markdown","source":["第二步：原始数据加载"],"metadata":{"id":"nL9bVauLndcn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rr7lIPVueNJr","collapsed":true},"outputs":[],"source":["dataset = load_dataset(\"glue\", \"mrpc\")"]},{"cell_type":"markdown","source":["第三步：数据处理"],"metadata":{"id":"AnGEBuZhnlW3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUuc1TSLjwMR","collapsed":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n","\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer)"]},{"cell_type":"markdown","source":["第四步：构建模型"],"metadata":{"id":"-cqb3IdrnwzX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7v2Q603wUJW"},"outputs":[],"source":["model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=2)\n","metric = evaluate.load(\"accuracy\")\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                  eval_strategy=\"epoch\",\n","                  per_device_train_batch_size=8,\n","                  per_device_eval_batch_size=8,\n","                  num_train_epochs=5,\n","                  logging_steps=10,\n","                  )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['validation'],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","source":["第五步：模型保存与加载"],"metadata":{"id":"ga5J1SJVCYIq"}},{"cell_type":"code","source":["trainer.save_model('./my_model/')"],"metadata":{"id":"5qCYRXrgtkcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\"./my_model/\")"],"metadata":{"id":"Rzn2L3j6Dyge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["第六步：模型推理"],"metadata":{"id":"-vuzvjjPCi_1"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# 加载文本分类pipeline\n","classifier = pipeline(\"text-classification\", model=\"./my_model/\", tokenizer=\"google-bert/bert-base-cased\", device=0)\n","\n","# 输入文本进行推理\n","texts = {\"text\": \"this is a good thing for me\", \"text_pair\": \"it is a good news\"}\n","results = classifier(texts)\n","\n","# 输出结果\n","print(results)\n","# for result in results:\n","#     print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")"],"metadata":{"id":"xrwqA6hWFuy0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"16NmdOis38oX_OuiaDdyxAJRIiBmobPHl","timestamp":1724065213936}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMf4Ky8gQ1g/vgBUT6g/zX5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}