{"cells":[{"cell_type":"markdown","source":["第一步：安装和导入必要包"],"metadata":{"id":"uj7dQUHam05d"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install evaluate"],"metadata":{"collapsed":true,"id":"C2t_eoaImxz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"iw1v_mUqiyHq"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import numpy as np\n","import evaluate"]},{"cell_type":"markdown","source":["第二步：原始数据加载"],"metadata":{"id":"nL9bVauLndcn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rr7lIPVueNJr","collapsed":true},"outputs":[],"source":["dataset = load_dataset(\"yelp_review_full\")"]},{"cell_type":"markdown","source":["第三步：数据处理"],"metadata":{"id":"AnGEBuZhnlW3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUuc1TSLjwMR","collapsed":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n","small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"]},{"cell_type":"markdown","source":["第四步：构建模型"],"metadata":{"id":"-cqb3IdrnwzX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7v2Q603wUJW"},"outputs":[],"source":["model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n","metric = evaluate.load(\"accuracy\")\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\",\n","                  eval_strategy=\"epoch\",\n","                  per_device_train_batch_size=8,\n","                  per_device_eval_batch_size=8,\n","                  logging_steps=10,\n","                  )\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_train_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","source":["第五步：模型保存与加载"],"metadata":{"id":"ga5J1SJVCYIq"}},{"cell_type":"code","source":["trainer.save_model('./my_model/')"],"metadata":{"id":"5qCYRXrgtkcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\"./my_model/\")"],"metadata":{"id":"Rzn2L3j6Dyge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["第六步：模型推理"],"metadata":{"id":"-vuzvjjPCi_1"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# 加载文本分类pipeline\n","classifier = pipeline(\"text-classification\", model=\"./my_model/\", tokenizer=\"google-bert/bert-base-cased\", device=0)\n","\n","# 输入文本进行推理\n","texts = [\"这是第一个示例文本。\", \"这是第二个示例文本。\"]\n","results = classifier(texts)\n","\n","# 输出结果\n","for result in results:\n","    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")"],"metadata":{"id":"xrwqA6hWFuy0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOmQWFTBTDe0HYgiUdo+xz+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}